{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mind model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:40:45.490725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-27 14:40:45.490818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-27 14:40:45.507397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 14:40:45.814796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 14:40:50.545927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Any\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization\n",
    "from keras.metrics import BinaryCrossentropy\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image as Img, ImageOps\n",
    "from PIL.Image import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hardware Requirements**:\n",
    "- At least 16 GB of RAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../data/dataset/\"\n",
    "MODEL_DIR = \"../data/model/\"\n",
    "MODEL_NAME = \"mind.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempts to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, the GPU memory region is extended for the TensorFlow process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:40:54.162655: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:40:55.053279: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:40:55.053324: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:40:55.097371: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:40:55.097420: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:40:55.097441: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:41:02.742990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:41:02.743225: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:41:02.743237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-27 14:41:02.743297: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-27 14:41:02.743497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5407 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "GPU_LIST = tf.config.list_physical_devices(\"GPU\")\n",
    "if GPU_LIST:\n",
    "    try:\n",
    "        for gpu in GPU_LIST:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpu_list = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(f\"Available GPUs: {len(GPU_LIST)} Physical GPUs, {len(logical_gpu_list)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Helper function to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory: str) -> tuple[list[Image], list[Image]]:\n",
    "    image_paths = [os.path.join(directory, file_name) for file_name in os.listdir(directory) if \"_mask\" not in file_name]\n",
    "    mask_paths = [image_path.replace(\".tif\", \"_mask.tif\") for image_path in image_paths]\n",
    "    images = [Img.open(image_path) for image_path in image_paths]\n",
    "    masks = [Img.open(mask_path) for mask_path in mask_paths]\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Helper function to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images: list[Image], masks: list[Image], second_split_percentage: float) -> tuple[list[Image], list[Image], list[Image], list[Image]]:\n",
    "    if len(images) != len(masks):\n",
    "        raise ValueError(\"Length of images and masks must be the same.\")\n",
    "    \n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Combine images and masks into pairs\n",
    "    data_pairs = list(zip(images, masks))\n",
    "\n",
    "    # Shuffle the data pairs\n",
    "    random.shuffle(data_pairs)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_num = int(len(data_pairs) * (1 - second_split_percentage))\n",
    "\n",
    "    # Split data\n",
    "    split_1_data = data_pairs[:split_num]\n",
    "    split_2_data = data_pairs[split_num:]\n",
    "\n",
    "    # Unzip\n",
    "    split_1_images, split_1_masks = zip(*split_1_data)\n",
    "    split_2_images, split_2_masks = zip(*split_2_data)\n",
    "\n",
    "    return split_1_images, split_1_masks, split_2_images, split_2_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Split the data into Train, Validate and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_masks, tmp_images, tmp_masks = split_data(images, masks, 0.2)\n",
    "validation_images, validation_masks, test_images, test_masks = split_data(tmp_images, tmp_masks, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Augment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificially increase the size of a dataset by applying various transformations to the existing data samples. The goal is to diversify the dataset and improve the generalization and robustness of a ML model. Data augmentation is commonly used in computer vision tasks, such as image classification and object detection, to create variations of the input data without collecting new samples.\n",
    "\n",
    "The validation and test set is used to try to estimate how your method works on real world data, thus it should only contain real world data. Adding augmented data will to not improve the accuracy of the validation. It will at best say something about how well your method responds to the data augmentation, and at worst ruin the validation results and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(eñaut): Ikusi nola egin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Clip mask values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel in the mask has a value of either 0 or 255. This means, that if the pixel value is 0, the same pixel position in the associated image is not part of a tumor, and if its 255, then it is part of a tumor. To better represent that, the mask is converted to an array of boolean values, by replacing anything greater than 1 as 1, and then changing the datatype of the elements of the ndarray to a boolean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Helper function to clip masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(mask: Image) -> ndarray:\n",
    "    return np.array(mask).clip(max=1).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Clip masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [clip(mask) for mask in train_masks]\n",
    "validation_masks = [clip(mask) for mask in validation_masks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normalize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all pixel value range to [0, 1]. The primary reasons for normalizing images in CNNs are:\n",
    "\n",
    "1. **Improved Convergence**: Normalizing images helps the optimization algorithm converge faster during training. Neural networks often perform better when the input data has zero mean and a small standard deviation. Normalization brings the pixel values to a common scale, preventing large input values from dominating the learning process. This can lead to faster convergence and more stable training.\n",
    "\n",
    "2. **Gradient Descent Stability**: During backpropagation, the optimization algorithm adjusts the weights of the neural network based on the gradients of the loss with respect to the weights. Normalizing the input data helps ensure that the gradients are within a reasonable range. This can prevent issues like exploding or vanishing gradients, which can hinder the training process.\n",
    "\n",
    "3. **Model Robustness**: Normalization can make the model more robust to variations in illumination and contrast. By bringing the pixel values to a standard scale, the network becomes less sensitive to changes in lighting conditions or differences in pixel intensity across different images.\n",
    "\n",
    "4. **Generalization**: Normalization aids in generalization by making the model less dependent on the specific characteristics of the training data. It allows the model to learn patterns and features that are more transferable across different datasets.\n",
    "\n",
    "5. **Compatibility with Activation Functions**: Some activation functions, such as the sigmoid and tanh functions, perform better when the input values are within a certain range. Normalizing the data helps ensure that the inputs to these functions fall within the regions where they exhibit desirable properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Helper function to normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image: Image) -> ndarray:\n",
    "    return np.array(image) / 255.0          \n",
    "\n",
    "\n",
    "# Galdetu normalizazio motak, eta zergatik modeloek 0 centered izatea gustatzen zaien\n",
    "\n",
    "# def normalize(image) -> ndarray:\n",
    "#     # Convert PIL Image to a NumPy array\n",
    "#     image_array = np.array(image)\n",
    "\n",
    "#     # Normalize the image array (subtract mean and divide by standard deviation)\n",
    "#     normalized_image_array = (image_array - np.mean(image_array)) / np.std(image_array)\n",
    "\n",
    "#     # Convert the normalized array back to a PIL Image\n",
    "#     normalized_image = Image.fromarray((normalized_image_array * 255).astype(np.uint8))\n",
    "\n",
    "#     return normalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = [normalize(image) for image in train_images]\n",
    "validation_images = [normalize(image) for image in validation_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create U-Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idatzi zergatik U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Helper function to create U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (eñaut): Refactor\n",
    "def unet_builder(input_size = (256,256,3)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2DTranspose(512,2,strides=(2,2),padding='same')(drop5)\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2DTranspose(256,2,strides=(2,2),padding='same')(conv6)\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2DTranspose(128,2,strides=(2,2),padding='same')(conv7)\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2DTranspose(64,2,strides=(2,2),padding='same')(conv8)\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss='binary_crossentropy', metrics=['accuracy',BinaryCrossentropy()])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Create U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 32, 32, 512)          0         ['conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['dropout[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 16, 16, 1024)         0         ['conv2d_9[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 32, 32, 512)          2097664   ['dropout_1[0][0]']           \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 32, 32, 1024)         0         ['dropout[0][0]',             \n",
      "                                                                     'conv2d_transpose[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 32, 32, 512)          4719104   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 64, 64, 256)          524544    ['conv2d_11[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_5[0][0]',            \n",
      " )                                                                   'conv2d_transpose_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 64, 64, 256)          1179904   ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['conv2d_13[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_3[0][0]',            \n",
      " )                                                                   'conv2d_transpose_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['conv2d_15[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_1[0][0]',            \n",
      " )                                                                   'conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['conv2d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 256, 256, 2)          1154      ['conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 256, 256, 1)          3         ['conv2d_18[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31032837 (118.38 MB)\n",
      "Trainable params: 31032837 (118.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_model = unet_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:44:54.699040: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2471755776 exceeds 10% of free system memory.\n",
      "2023-12-27 14:45:18.093853: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 205979648 exceeds 10% of free system memory.\n",
      "2023-12-27 14:45:19.838722: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2471755776 exceeds 10% of free system memory.\n",
      "2023-12-27 14:45:22.314539: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 205979648 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:45:28.896010: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-12-27 14:45:32.348658: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-27 14:45:37.308229: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-27 14:45:39.971168: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-27 14:45:42.883326: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:43.004849: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:43.064729: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.70GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:43.161197: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.70GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:44.280889: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fefcf7c2610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-27 14:45:44.280930: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2023-12-27 14:45:44.324675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703684744.452967  160432 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-12-27 14:45:46.062817: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:46.062911: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:46.328524: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:46.328604: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:46.899399: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 848.70MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-12-27 14:45:47.357012: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 848.70MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9896 - binary_crossentropy: 0.1620"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:49:25.081831: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 309067776 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - 256s 549ms/step - loss: 0.1620 - accuracy: 0.9896 - binary_crossentropy: 0.1620 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 2/20\n",
      "393/393 [==============================] - 167s 426ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 3/20\n",
      "393/393 [==============================] - 167s 425ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 4/20\n",
      "393/393 [==============================] - 168s 427ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 5/20\n",
      "393/393 [==============================] - 173s 441ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 6/20\n",
      "393/393 [==============================] - 185s 470ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 7/20\n",
      "393/393 [==============================] - 179s 457ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 8/20\n",
      "393/393 [==============================] - 181s 460ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 9/20\n",
      "393/393 [==============================] - 154s 391ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 10/20\n",
      "393/393 [==============================] - 155s 394ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 11/20\n",
      "393/393 [==============================] - 154s 392ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 12/20\n",
      "393/393 [==============================] - 151s 384ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 13/20\n",
      "393/393 [==============================] - 159s 404ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 14/20\n",
      "393/393 [==============================] - 162s 412ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 15/20\n",
      "393/393 [==============================] - 159s 404ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 16/20\n",
      "393/393 [==============================] - 160s 406ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 17/20\n",
      "393/393 [==============================] - 164s 419ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 18/20\n",
      "393/393 [==============================] - 164s 416ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 19/20\n",
      "393/393 [==============================] - 162s 410ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n",
      "Epoch 20/20\n",
      "393/393 [==============================] - 156s 397ms/step - loss: 0.1597 - accuracy: 0.9896 - binary_crossentropy: 0.1597 - val_loss: 0.1655 - val_accuracy: 0.9893 - val_binary_crossentropy: 0.1655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff345b798d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TensorBoard callback\n",
    "from datetime import datetime\n",
    "\n",
    "logs = \"../logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tboard_callback = TensorBoard(log_dir = logs, histogram_freq = 1, profile_batch = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.fit(train_images, train_masks, epochs=20, validation_data=(validation_images, validation_masks), batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.save(MODEL_DIR + MODEL_NAME, save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Select model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the saved model, or the one already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False\n",
    "unet_model: Any\n",
    "if LOAD_MODEL:\n",
    "    unet_model = load_model(MODEL_DIR + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Preprocess test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, the same preprocessing, that the validation and train data had, will be applied to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Clip masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_masks = [clip(mask) for mask in test_masks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [normalize(image) for image in test_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model using the Accuracy and BinaryCrossentropy metrics, defined in the model creation, with unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 20s 710ms/step - loss: 0.1451 - accuracy: 0.9906 - binary_crossentropy: 0.1451\n",
      "Loss: 0.1451345980167389, Accuracy: 0.9905909299850464\n"
     ]
    }
   ],
   "source": [
    "evaluation = unet_model.evaluate(np.array(test_images), np.array(test_masks))\n",
    "print(f\"Loss: {evaluation[0]}, Accuracy: {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Visual evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = unet_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 Plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(test_images[i])\n",
    "    plt.title(\"Input image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(test_masks[i])\n",
    "    plt.title(\"Real mask\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predictions[i, :, :, 0], cmap=\"gray\")\n",
    "    plt.title(\"Predicted mask\")\n",
    "\n",
    "    plt.show()\n",
    "    # plt.savefig(f\"fig_{i}.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
